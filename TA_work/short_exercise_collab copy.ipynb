{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "781a36c9",
   "metadata": {},
   "source": [
    "This repo is based on DiffScaler, https://github.com/DSIP-FBK/DiffScaler/tree/main/src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f60f02f",
   "metadata": {},
   "source": [
    "You are already acquainted with Pytorch now as a fanstastic library for writing, modifying, testing and scaling up the code. In this notebook, let us learn a cool wrapper library for Pytorch known as Pytorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9556bf29",
   "metadata": {},
   "source": [
    "A great resource to learn about the differences between Pytorch and Pytorch Lightning and how Pytorch Lightning makes your life easier : https://www.geeksforgeeks.org/deep-learning/pytorch-vs-pytorch-lightning/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee6fee",
   "metadata": {},
   "source": [
    "Please note that according to this TA, you can`t really appreciate the resurcefulness of Pytorch lightning without learning the basics of Pytorch. Hence, take this tutorial as a 10,000 ft overview of what Pytorch and Pytorch lightning actually do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1041bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from lightning import LightningModule, LightningDataModule, Trainer\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "import hydra\n",
    "import random\n",
    "import zstandard\n",
    "import os\n",
    "import glob\n",
    "import io\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "import rioxarray\n",
    "import json\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78b635d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import load_and_normalise, decompress_zst_pt, get_file_list, collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375d5e12",
   "metadata": {},
   "source": [
    "Training the hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e840f325",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg=OmegaConf.load(\"conf/config_experiments.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3785db5",
   "metadata": {},
   "source": [
    "Writing the YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(cfg, input_sample, ground_truth=None):\n",
    "    stats_path = os.path.join(cfg.paths.output_dir, \"stats.json\")\n",
    "    with open(stats_path, \"r\") as f:\n",
    "        stats = json.load(f)\n",
    "    low_2mt_mean = stats[\"low_2mt_mean\"]\n",
    "    low_2mt_std = stats[\"low_2mt_std\"]\n",
    "\n",
    "    def denormalise(arr):\n",
    "        return arr * low_2mt_std + low_2mt_mean\n",
    "\n",
    "    # Load ckpts for UNet, VAE, LDM\n",
    "    checkpoint_dir = cfg.paths.checkpoint_dir\n",
    "    unet_module = UNetLitModule.load_from_checkpoint(\n",
    "        os.path.join(checkpoint_dir, \"unet\", sorted(os.listdir(checkpoint_dir + \"/unet\"))[-1])\n",
    "    )\n",
    "    vae_module = VAELitModule.load_from_checkpoint(\n",
    "        os.path.join(checkpoint_dir, \"vae\", sorted(os.listdir(checkpoint_dir + \"/vae\"))[-1])\n",
    "    )\n",
    "    ldm_module = LDMLitModule.load_from_checkpoint(\n",
    "        os.path.join(checkpoint_dir, \"ldm\", sorted(os.listdir(checkpoint_dir + \"/ldm\"))[-1]),\n",
    "        vae=vae_module\n",
    "    )\n",
    "\n",
    "    unet_module.eval()\n",
    "    vae_module.eval()\n",
    "    ldm_module.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        device = next(unet_module.parameters()).device\n",
    "        fuzzy_input = input_sample.unsqueeze(0).to(device)\n",
    "        unet_pred = unet_module.net(fuzzy_input)\n",
    "\n",
    "        input_height = vae_module.hparams.input_height // 2\n",
    "        input_width = vae_module.hparams.input_width // 2\n",
    "        latent_shape = (1, 1, input_height, input_width)\n",
    "\n",
    "        # Downsample unet_pred to latent spatial size\n",
    "        unet_pred_ds = F.interpolate(unet_pred, size=(input_height, input_width), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Generate 3 samples\n",
    "        generated_latents = []\n",
    "        for _ in range(3):\n",
    "            z_sample = ldm_module.sample(latent_shape, unet_pred_ds)\n",
    "            z_sample_flat = z_sample.view(z_sample.size(0), -1)\n",
    "            generated_latents.append(z_sample_flat)\n",
    "\n",
    "        generated_latents = torch.cat(generated_latents, dim=0)\n",
    "\n",
    "        # Decode latents to residuals\n",
    "        generated_residuals = vae_module.model.decode(generated_latents, unet_pred)\n",
    "\n",
    "        final_reconstructions = unet_pred + generated_residuals\n",
    "\n",
    "    # Prepare images for plotting\n",
    "    all_imgs = [\n",
    "        denormalise(fuzzy_input[0, 0].cpu().numpy()),  # ERA5 predictor\n",
    "        denormalise(final_reconstructions[0, 0].cpu().numpy()),  # Sample 1\n",
    "        denormalise(final_reconstructions[1, 0].cpu().numpy()),  # Sample 2\n",
    "        denormalise(final_reconstructions[2, 0].cpu().numpy()),  # Sample 3\n",
    "    ]\n",
    "    titles = [\n",
    "        \"ERA5 2m input\",\n",
    "        \"Sample 1\",\n",
    "        \"Sample 2\",\n",
    "        \"Sample 3\",\n",
    "    ]\n",
    "\n",
    "    # Add ground truth if provided\n",
    "    if ground_truth is not None:\n",
    "        all_imgs.append(denormalise(ground_truth.cpu().numpy()))\n",
    "        titles.append(\"Ground Truth\")\n",
    "\n",
    "    vmin = min(img.min() for img in all_imgs)\n",
    "    vmax = max(img.max() for img in all_imgs)\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(all_imgs), figsize=(6 * len(all_imgs), 6), constrained_layout=True)\n",
    "    images = []\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        im = ax.imshow(all_imgs[i], cmap='coolwarm', vmin=vmin, vmax=vmax, origin='lower')\n",
    "        ax.set_title(titles[i], fontsize=15, fontweight='bold')\n",
    "        ax.set_xlabel(\"Longitude\", fontsize=13)\n",
    "        ax.set_ylabel(\"Latitude\", fontsize=13)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=11)\n",
    "        ax.grid(False, which='both')\n",
    "        images.append(im)\n",
    "\n",
    "    cbar = fig.colorbar(images[0], ax=axes, orientation='horizontal', fraction=0.04, pad=0.08)\n",
    "    cbar.set_label(\"2m Temperature (C)\", fontsize=15, fontweight='bold')\n",
    "    cbar.ax.tick_params(labelsize=13)\n",
    "\n",
    "    plt.suptitle(\"ERA5 low res + 3 samples + Ground Truth (conditional inference)\", fontsize=20, y=1.05, fontweight='bold')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d4f473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 48 samples from 2020.\n",
      "Using 48 samples from 2020.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'lon'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Now that the models are trained,,, how will you perform inference using the trained hierarchy?\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 92\u001b[0m, in \u001b[0;36minference\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# latlon extent for plotting\u001b[39;00m\n\u001b[1;32m     91\u001b[0m lat \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 92\u001b[0m lon \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     93\u001b[0m lat_min, lat_max \u001b[38;5;241m=\u001b[39m lat\u001b[38;5;241m.\u001b[39mmin(), lat\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m     94\u001b[0m lon_min, lon_max \u001b[38;5;241m=\u001b[39m lon\u001b[38;5;241m.\u001b[39mmin(), lon\u001b[38;5;241m.\u001b[39mmax()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lon'"
     ]
    }
   ],
   "source": [
    "#Now that the models are trained,,, how will you perform inference using the trained hierarchy?\n",
    "inference(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffscaler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
