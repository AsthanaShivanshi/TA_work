{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "781a36c9",
   "metadata": {},
   "source": [
    "This repo is based on DiffScaler, https://github.com/DSIP-FBK/DiffScaler/tree/main/src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f60f02f",
   "metadata": {},
   "source": [
    "You are already acquainted with Pytorch now as a fanstastic library for writing, modifying, testing and scaling up the code. In this notebook, let us learn a cool wrapper library for Pytorch known as Pytorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9556bf29",
   "metadata": {},
   "source": [
    "A great resource to learn about the differences between Pytorch and Pytorch Lightning and how Pytorch Lightning makes your life easier : https://www.geeksforgeeks.org/deep-learning/pytorch-vs-pytorch-lightning/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee6fee",
   "metadata": {},
   "source": [
    "Please note that according to this TA, you can`t really appreciate the resurcefulness of Pytorch lightning without learning the basics of Pytorch. Hence, take this tutorial as a 10,000 ft overview of what Pytorch and Pytorch lightning actually do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1041bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from lightning import LightningModule, LightningDataModule, Trainer\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "import hydra\n",
    "import random\n",
    "import zstandard\n",
    "import os\n",
    "import glob\n",
    "import io\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "import rioxarray\n",
    "import json\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515752fd",
   "metadata": {},
   "source": [
    "The original paper this notebook is based on, can be accessed at https://gmd.copernicus.org/articles/18/2051/2025/ . There are originally 14 input channels to predict 2 chanels ultimately : temperature and wind speed. For the purposes of this notebook (due to storage limits and memory issues), we shall only be using low res 2m temperature to predict the high res 2m temperature target field, using hourly data from a single year (2020). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f58108d",
   "metadata": {},
   "source": [
    " But to supplement our endeavours in the form of giving more data to the model, we will also feed in static inputs (inputs that do not change or evolve over time) which are 1. digital elevation model (DEM), 2. land cover categories and 3. latitudinal bands. The functions to do so can be found in the preprocessing file, and we import them here for ease of analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78b635d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import load_and_normalise, decompress_zst_pt, get_file_list, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1105b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "class DownscalingDataset(Dataset):\n",
    "    def __init__(self, file_list, static_vars, low_2mt_mean, low_2mt_std):\n",
    "        self.file_list = file_list  # List of (high_file, low_file, date)\n",
    "        self.static_vars = static_vars  # for a list of static variables, refer the preprocessing file which loads and normalises these datasets\n",
    "        self.low_2mt_mean = low_2mt_mean\n",
    "        self.low_2mt_std = low_2mt_std\n",
    "        self.samples = []\n",
    "        # Each file contains 24 hourly samples\n",
    "        for hf, lf, date in self.file_list:\n",
    "            for hour in range(24):\n",
    "                self.samples.append((hf, lf, hour))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hf, lf, hour = self.samples[idx]\n",
    "        # Loading here high-res and low-res data for the given hour\n",
    "        high_data = decompress_zst_pt(hf)\n",
    "        low_data = decompress_zst_pt(lf)\n",
    "        # the high res and low res (upsampled) low res files are being loaded here\n",
    "        high_t2m = high_data[hour][\"2mT\"].float()  #[672, 576]\n",
    "        low_t2m = low_data[hour][\"2mT\"].float()    #[84, 72]\n",
    "\n",
    "\n",
    "        high_t2m = high_t2m.unsqueeze(0)  #[1, 672, 576]\n",
    "        low_t2m = low_t2m.unsqueeze(0)    #[1, 84, 72]\n",
    "        # Static vars\n",
    "        dem = self.static_vars[\"dem\"].unsqueeze(0)  #[1, 672, 576]\n",
    "        lat = self.static_vars[\"lat\"].unsqueeze(0)  #[1, 672, 576]\n",
    "        lc = self.static_vars[\"lc\"]                 #[bands, 672, 576] ,,,by bands we mean land cover bands here\n",
    "\n",
    "        #Finally, this returns a dictionary with the following keys and values (variables)\n",
    "        return {\n",
    "            \"low_2mT\": low_t2m,        #[1, 84, 72]\n",
    "            \"high_2mT\": high_t2m,      #[1, 672, 576]\n",
    "            \"dem\": dem,                #[1, 672, 576]\n",
    "            \"lat\": lat,                #[1, 672, 576]\n",
    "            \"lc\": lc                   #[bands, 672, 576]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9cb8000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "class DownscalingDataModule(LightningDataModule):\n",
    "    def __init__(self, batch_size, val_frac, test_frac, num_workers, static_dir, save_stats_json):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.val_frac = val_frac\n",
    "        self.test_frac = test_frac\n",
    "        self.num_workers = num_workers\n",
    "        self.static_dir = static_dir\n",
    "        self.save_stats_json = save_stats_json\n",
    "\n",
    "    def setup(self, stage=None):  \n",
    "        static_vars, stats = load_and_normalise(\n",
    "            self.static_dir,\n",
    "            val_frac=self.val_frac,\n",
    "            test_frac=self.test_frac,\n",
    "            save_stats_json=self.save_stats_json\n",
    "        )\n",
    "        file_list = get_file_list()\n",
    "        dataset = DownscalingDataset(\n",
    "            file_list, static_vars,\n",
    "            low_2mt_mean=stats[\"low_2mt_mean\"],\n",
    "            low_2mt_std=stats[\"low_2mt_std\"]\n",
    "        )\n",
    "        N = len(dataset)\n",
    "        n_val = int(self.val_frac * N)\n",
    "        n_test = int(self.test_frac * N)\n",
    "        n_train = N - n_val - n_test\n",
    "\n",
    "        # Contiguous splits (no shuffling)\n",
    "        self.train_set = torch.utils.data.Subset(dataset, range(0, n_train))\n",
    "        self.val_set = torch.utils.data.Subset(dataset, range(n_train, n_train + n_val))\n",
    "        self.test_set = torch.utils.data.Subset(dataset, range(n_train + n_val, N))\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, collate_fn=collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, collate_fn=collate_fn)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, collate_fn=collate_fn)\n",
    "\n",
    "    @property\n",
    "    def test_dataset(self):\n",
    "        return self.test_set.dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9440c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, channels, kernel_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels[0], kernel_size, padding=1)\n",
    "        self.conv2 = nn.Conv2d(channels[0], channels[1], kernel_size, padding=1)\n",
    "        self.final = nn.Conv2d(channels[1], out_channels, 1)\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.relu(self.conv2(h))\n",
    "        output = self.final(h)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65978fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualsDataModule(LightningDataModule):\n",
    "    def __init__(self, unet_preds, residuals, batch_size):\n",
    "        super().__init__()\n",
    "        self.unet_preds = unet_preds  #Needed for conditonal generation\n",
    "        self.residuals = residuals\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.dataset = TensorDataset(self.unet_preds, self.residuals)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.dataset, batch_size=self.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5068d396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(mean, log_var):\n",
    "    kl = 0.5 * (log_var.exp() + mean.square() - 1.0 - log_var)\n",
    "    return kl.mean()\n",
    "\n",
    "def standard_normal_sampling(mean, log_var, num=None):\n",
    "    std = log_var.mul(0.5).exp()\n",
    "    shape = mean.shape\n",
    "    if num is not None:\n",
    "        shape = shape[:1] + (num,) + shape[1:]\n",
    "        mean = mean[:, None, ...]\n",
    "        std = std[:, None, ...]\n",
    "    return mean + std * torch.randn(shape, device=mean.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d702e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetLitModule(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        net: torch.nn.Module = None,\n",
    "        lr: float = 1e-3,\n",
    "        optimizer: dict = None,\n",
    "        scheduler: dict = None,\n",
    "        loss_fn: torch.nn.Module = None,\n",
    "        in_channels: int = 19,\n",
    "        out_channels: int = 1,\n",
    "        channels: list = [32, 16],\n",
    "        kernel_size: int = 3,\n",
    "        ckpt_path: str = None,\n",
    "        ignore_keys: list = []\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(logger=False, ignore=['net', 'loss_fn'])\n",
    "        self.net = net if net is not None else UNet(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            channels=channels,\n",
    "            kernel_size=kernel_size\n",
    "        )\n",
    "        self.loss_fn = loss_fn if loss_fn is not None else nn.MSELoss()\n",
    "        self.lr = lr\n",
    "        self.hparams.optimizer = optimizer\n",
    "        self.hparams.scheduler = scheduler\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def model_step(self, batch):\n",
    "        fuzzy_input, sharp_target = batch\n",
    "        pred = self.forward(fuzzy_input)\n",
    "        loss = self.loss_fn(pred, sharp_target)\n",
    "        return loss, pred\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, pred = self.model_step(batch)\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, pred = self.model_step(batch)\n",
    "        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, pred = self.model_step(batch)\n",
    "        self.log(\"test/loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_cfg = self.hparams.get(\"optimizer\", None)\n",
    "        sch_cfg = self.hparams.get(\"scheduler\", None)\n",
    "\n",
    "        if opt_cfg and opt_cfg[\"type\"] == \"AdamW\":\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                self.parameters(),\n",
    "                lr=opt_cfg.get(\"lr\", self.lr),\n",
    "                betas=tuple(opt_cfg.get(\"betas\", (0.5, 0.9))),\n",
    "                weight_decay=opt_cfg.get(\"weight_decay\", 1e-3)\n",
    "            )\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(\n",
    "                self.parameters(),\n",
    "                lr=opt_cfg.get(\"lr\", self.lr) if opt_cfg else self.lr,\n",
    "                weight_decay=opt_cfg.get(\"weight_decay\", 1e-4) if opt_cfg else 1e-4\n",
    "            )\n",
    "\n",
    "        if sch_cfg and sch_cfg[\"type\"] == \"ReduceLROnPlateau\":\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                patience=sch_cfg.get(\"patience\", 2),\n",
    "                factor=sch_cfg.get(\"factor\", 0.25)\n",
    "            )\n",
    "            return {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": scheduler,\n",
    "                    \"monitor\": sch_cfg.get(\"monitor\", \"val/loss\"),\n",
    "                    \"frequency\": 1,\n",
    "                },\n",
    "            }\n",
    "        else:\n",
    "            return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad1baa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=32, input_channels=2, input_height=672, input_width=576, encoder_channels=[8], decoder_channels=[8]):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, encoder_channels[0], 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2), nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        encoded_size = encoder_channels[0] * (input_height // 2) * (input_width // 2)\n",
    "        self.fc_mu = nn.Linear(encoded_size, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(encoded_size, latent_dim)\n",
    "        self.fc_decode = nn.Linear(latent_dim, encoded_size)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, (decoder_channels[0], input_height // 2, input_width // 2)),\n",
    "            nn.ConvTranspose2d(decoder_channels[0] + 1, 1, 2, stride=2),  # +1 for unet_pred\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = (0.5 * logvar).exp()\n",
    "        return mu + std * torch.randn_like(std)\n",
    "\n",
    "    def decode(self, z, unet_pred):\n",
    "        h = self.fc_decode(z)\n",
    "        h = h.view(z.size(0), -1, unet_pred.shape[-2] // 2, unet_pred.shape[-1] // 2)\n",
    "        # Downsample unet_pred to match h spatial size\n",
    "        unet_pred_ds = F.interpolate(unet_pred, size=h.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        # Concatenate along channel dimension\n",
    "        h_cat = torch.cat([h, unet_pred_ds], dim=1)\n",
    "        return self.decoder(h_cat)\n",
    "\n",
    "    def forward(self, x, unet_pred, sample_posterior=True):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar) if sample_posterior else mu\n",
    "        recon = self.decode(z, unet_pred)\n",
    "        return recon, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f1d2eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAELitModule(LightningModule):\n",
    "    def __init__(self, latent_dim=32, lr=1e-3, kl_weight=0.001, input_channels=2, input_height=672, input_width=576, encoder_channels=[8], decoder_channels=[8], optimizer=None, scheduler=None, unet_module=None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(logger=False, ignore=['unet_module'])\n",
    "        self.model = VAE(\n",
    "            latent_dim=latent_dim,\n",
    "            input_channels=input_channels,\n",
    "            input_height=input_height,\n",
    "            input_width=input_width,\n",
    "            encoder_channels=encoder_channels,\n",
    "            decoder_channels=decoder_channels,\n",
    "        )\n",
    "        self.lr = lr\n",
    "        self.kl_weight = kl_weight\n",
    "        self.hparams.optimizer = optimizer\n",
    "        self.hparams.scheduler = scheduler\n",
    "        self.unet = unet_module   #Note that this is required have conditional generation!\n",
    "\n",
    "    def forward(self, x, sample_posterior=True):\n",
    "        \n",
    "        return self.model(x, sample_posterior)\n",
    "    \n",
    "    def _loss(self, batch):\n",
    "        fuzzy_input, sharp_target = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            unet_pred = self.unet(fuzzy_input)\n",
    "        residual = sharp_target - unet_pred\n",
    "        # Only use the coarse channel for VAE encoder\n",
    "        encoder_input = torch.cat([unet_pred[:, 0:1], residual], dim=1)\n",
    "        recon, mu, logvar = self.model(encoder_input, unet_pred[:, 0:1])\n",
    "        recon_loss = F.l1_loss(recon, residual)\n",
    "        kl_loss = kl_divergence(mu, logvar)\n",
    "        total_loss = recon_loss + self.kl_weight * kl_loss\n",
    "        return total_loss, recon_loss, kl_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        total_loss, recon_loss, kl_loss = self._loss(batch)\n",
    "        self.log(\"train/loss\", total_loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"train/recon_loss\", recon_loss, on_step=True, on_epoch=True, sync_dist=True)\n",
    "        self.log(\"train/kl_loss\", kl_loss, on_step=True, on_epoch=True, sync_dist=True)\n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        total_loss, recon_loss, kl_loss = self._loss(batch)\n",
    "        self.log(\"val/loss\", total_loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"val/recon_loss\", recon_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
    "        self.log(\"val/kl_loss\", kl_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
    "        return total_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        total_loss, recon_loss, kl_loss = self._loss(batch)\n",
    "        self.log(\"test/loss\", total_loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"test/recon_loss\", recon_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
    "        self.log(\"test/kl_loss\", kl_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
    "        return total_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_cfg = self.hparams.get(\"optimizer\", None)\n",
    "        sch_cfg = self.hparams.get(\"scheduler\", None)\n",
    "        if opt_cfg and opt_cfg.get(\"type\") == \"AdamW\":\n",
    "            optimizer = torch.optim.AdamW(self.parameters(), lr=opt_cfg.get(\"lr\", self.lr), betas=tuple(opt_cfg.get(\"betas\", (0.5, 0.9))), weight_decay=opt_cfg.get(\"weight_decay\", 1e-3))\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=opt_cfg.get(\"lr\", self.lr) if opt_cfg else self.lr, weight_decay=opt_cfg.get(\"weight_decay\", 1e-4) if opt_cfg else 1e-4)\n",
    "        if sch_cfg and sch_cfg.get(\"type\") == \"ReduceLROnPlateau\":\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=sch_cfg.get(\"patience\", 3), factor=sch_cfg.get(\"factor\", 0.25))\n",
    "            return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": sch_cfg.get(\"monitor\", \"val/recon_loss\"), \"frequency\": 1}}\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a19c3e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentDenoiser(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, channels, kernel_size):\n",
    "        super().__init__()\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(1, channels[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channels[0], channels[0])\n",
    "        )\n",
    "\n",
    "\n",
    "        self.unet = UNet(\n",
    "            in_channels=in_channels + 2,  \n",
    "            out_channels=out_channels,\n",
    "            channels=channels,\n",
    "            kernel_size=kernel_size\n",
    "        )\n",
    "\n",
    "    def forward(self, z_noisy, unet_pred, timestep=None):\n",
    "        # z_noisy: [batch, channels, H, W]\n",
    "        # unet_pred: [batch, 1, H, W]\n",
    "        if timestep is None:\n",
    "            timestep = torch.zeros(z_noisy.shape[0], 1, device=z_noisy.device)\n",
    "        B, _, H, W = z_noisy.shape\n",
    "        t_embed = self.time_embed(timestep.float())  # [batch, channels[0]]\n",
    "        t_map = t_embed[:, :1].unsqueeze(-1).unsqueeze(-1).expand(-1, 1, H, W)\n",
    "        x = torch.cat([z_noisy, unet_pred, t_map], dim=1)\n",
    "        return self.unet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfbbf67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDMLitModule(LightningModule):\n",
    "    def __init__(self, vae, latent_dim=64, lr=1e-4, num_timesteps=50, noise_schedule=\"linear\", loss_type=\"l2\", hidden_dim=128, num_layers=4, optimizer=None, scheduler=None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(logger=False, ignore=['vae'])\n",
    "        self.vae = vae.model.requires_grad_(False)\n",
    "        self.vae.eval()\n",
    "        self.denoiser = LatentDenoiser(1, 1, [hidden_dim]*num_layers, 3)\n",
    "        self.lr = lr\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.loss_fn = {\"l1\": nn.L1Loss(), \"l2\": nn.MSELoss()}[loss_type]\n",
    "        self.register_noise_schedule(noise_schedule)\n",
    "        self.hparams.optimizer = optimizer\n",
    "        self.hparams.scheduler = scheduler\n",
    "\n",
    "    def register_noise_schedule(self, schedule=\"linear\"):\n",
    "        if schedule == \"linear\":\n",
    "            betas = torch.linspace(1e-4, 2e-2, self.num_timesteps)\n",
    "        elif schedule == \"cosine\":\n",
    "            timesteps = torch.arange(self.num_timesteps + 1) / self.num_timesteps\n",
    "            alphas = torch.cos(timesteps * torch.pi / 2) ** 2\n",
    "            alphas = alphas / alphas[0]\n",
    "            betas = 1 - alphas[1:] / alphas[:-1]\n",
    "            betas = torch.clamp(betas, 0, 0.999)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown schedule: {schedule}\")\n",
    "\n",
    "        #Setting up and storing noise schedules as buffers\n",
    "        alphas = 1 - betas #beta here is the variance schedule (controls the noise added at each step)\n",
    "        \n",
    "        #How do we know how much signal remains after each timestep? \n",
    "        #This is essential because we need to separate signal from the noise added (fraction)\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0) #cumprod : cumulative product of elements along a given dimension.\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1 - alphas_cumprod))\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        return (self.sqrt_alphas_cumprod[t].view(-1, 1) * x_start +\n",
    "                self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1) * noise)\n",
    "\n",
    "    def p_losses(self, x_start, unet_pred, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        x_noisy = self.q_sample(x_start, t, noise)\n",
    "        B, latent_dim = x_noisy.shape\n",
    "        H = int(latent_dim ** 0.5)\n",
    "        W = latent_dim // H\n",
    "        x_noisy_4d = x_noisy.view(B, 1, H, W)\n",
    "        # Downsample unet_pred to latent size\n",
    "        unet_pred_ds = F.interpolate(unet_pred, size=(H, W), mode='bilinear', align_corners=False)\n",
    "        predicted_noise = self.denoiser(x_noisy_4d, unet_pred_ds, t.unsqueeze(1).to(x_noisy.device))\n",
    "        return self.loss_fn(predicted_noise, noise.view(B, 1, H, W))\n",
    "\n",
    "    def forward(self, x, unet_pred):\n",
    "        with torch.no_grad():\n",
    "            mu, logvar = self.vae.encode(x)\n",
    "            z = self.vae.reparameterize(mu, logvar)\n",
    "        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device)\n",
    "        return self.p_losses(z, unet_pred, t)\n",
    "    \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        fuzzy_input, sharp_target = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            unet_pred = self.vae.unet(fuzzy_input)\n",
    "        residual = sharp_target - unet_pred\n",
    "        encoder_input = torch.cat([unet_pred, residual], dim=1)\n",
    "        loss = self.forward(encoder_input, unet_pred)\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.forward(batch[1])\n",
    "        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.forward(batch[1])\n",
    "        self.log(\"test/loss\", loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, shape, unet_pred, num_steps=50):\n",
    "        z = torch.randn(shape, device=self.device)\n",
    "        H, W = ... # set to latent spatial size\n",
    "        unet_pred_ds = F.interpolate(unet_pred, size=(H, W), mode='bilinear', align_corners=False)\n",
    "        timesteps = torch.linspace(self.num_timesteps-1, 0, num_steps, dtype=torch.long, device=self.device)\n",
    "        for i, t in enumerate(timesteps):\n",
    "            t_batch = t.repeat(shape[0])\n",
    "            predicted_noise = self.denoiser(z, unet_pred_ds, t_batch.unsqueeze(1))\n",
    "            alpha_t = self.alphas_cumprod[t]\n",
    "            alpha_t_prev = self.alphas_cumprod[t-1] if t > 0 else torch.tensor(1.0)\n",
    "            pred_x0 = (z - torch.sqrt(1 - alpha_t) * predicted_noise) / torch.sqrt(alpha_t)\n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(z) if i < len(timesteps) - 1 else 0\n",
    "                z = torch.sqrt(alpha_t_prev) * pred_x0 + torch.sqrt(1 - alpha_t_prev) * noise\n",
    "            else:\n",
    "                z = pred_x0\n",
    "        return z\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_samples(self, num_samples=1):\n",
    "        z_samples = self.sample((num_samples, self.hparams.latent_dim))\n",
    "        return self.vae.decode(z_samples)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_cfg = self.hparams.get(\"optimizer\", None)\n",
    "        sch_cfg = self.hparams.get(\"scheduler\", None)\n",
    "        if opt_cfg and opt_cfg.get(\"type\") == \"AdamW\":\n",
    "            optimizer = torch.optim.AdamW(self.parameters(), lr=opt_cfg.get(\"lr\", self.lr), betas=tuple(opt_cfg.get(\"betas\", (0.5, 0.9))), weight_decay=opt_cfg.get(\"weight_decay\", 1e-3))\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=opt_cfg.get(\"lr\", self.lr) if opt_cfg else self.lr, weight_decay=opt_cfg.get(\"weight_decay\", 1e-4) if opt_cfg else 1e-4)\n",
    "        if sch_cfg and sch_cfg.get(\"type\") == \"ReduceLROnPlateau\":\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=sch_cfg.get(\"patience\", 5), factor=sch_cfg.get(\"factor\", 0.5))\n",
    "            return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": sch_cfg.get(\"monitor\", \"val/loss\"), \"frequency\": 1}}\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375d5e12",
   "metadata": {},
   "source": [
    "Training the hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5a1040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hierarchy(cfg):\n",
    "    data_module = DownscalingDataModule(\n",
    "        batch_size=cfg.data.batch_size,\n",
    "        val_frac=cfg.data.val_split,\n",
    "        test_frac=cfg.data.test_split,\n",
    "        num_workers=cfg.data.num_workers,\n",
    "        static_dir=cfg.paths.static_dir,\n",
    "        save_stats_json=os.path.join(cfg.paths.output_dir, \"stats.json\")\n",
    "    )\n",
    "    data_module.setup()\n",
    "\n",
    "    checkpoint_dir = cfg.paths.checkpoint_dir\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Unet for mean pred\n",
    "    unet_ckpt_dir = os.path.join(checkpoint_dir, \"unet\")\n",
    "    os.makedirs(unet_ckpt_dir, exist_ok=True)\n",
    "    unet_ckpts = sorted([f for f in os.listdir(unet_ckpt_dir) if f.endswith(\".ckpt\")])\n",
    "    if unet_ckpts:\n",
    "        print(\"Loading pretrained UNet ckpt\")\n",
    "        unet_module = UNetLitModule.load_from_checkpoint(os.path.join(unet_ckpt_dir, unet_ckpts[-1]))\n",
    "    else:\n",
    "        print(\"Training UNet\")\n",
    "        unet_module = UNetLitModule(\n",
    "            net=UNet(\n",
    "                in_channels=cfg.model.unet.in_channels,\n",
    "                out_channels=cfg.model.unet.out_channels,\n",
    "                channels=cfg.model.unet.channels,\n",
    "                kernel_size=cfg.model.unet.kernel_size,\n",
    "            ),\n",
    "            lr=cfg.model.unet.lr,\n",
    "            optimizer=cfg.optimizer.unet,\n",
    "            scheduler=cfg.scheduler.unet,\n",
    "        )\n",
    "        unet_checkpoint = ModelCheckpoint(\n",
    "            dirpath=unet_ckpt_dir,\n",
    "            filename=\"unet-{epoch:02d}\",\n",
    "            save_top_k=1,\n",
    "            monitor=\"val/loss\",\n",
    "            mode=\"min\",\n",
    "        )\n",
    "        trainer_unet = Trainer(\n",
    "            max_epochs=cfg.training.unet_epochs,\n",
    "            accelerator=cfg.trainer.accelerator,\n",
    "            gradient_clip_val=cfg.trainer.get(\"gradient_clip_val\", 1.0),\n",
    "            enable_checkpointing=True,\n",
    "            logger=False,\n",
    "            callbacks=[unet_checkpoint],\n",
    "        )\n",
    "        trainer_unet.fit(unet_module, datamodule=data_module)\n",
    "\n",
    "    # Calculating residuals and also storing unet pred for conditoonal generation\n",
    "\n",
    "    unet_preds = []\n",
    "    residuals = []\n",
    "    fuzzy_inputs = []\n",
    "\n",
    "    for batch in data_module.train_dataloader():\n",
    "        fuzzy_input, sharp_target = batch\n",
    "        device = next(unet_module.parameters()).device\n",
    "        fuzzy_input = fuzzy_input.to(device)\n",
    "        sharp_target = sharp_target.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = unet_module.net(fuzzy_input)\n",
    "            residual = sharp_target - pred\n",
    "            # Save the full fuzzy_input for UNet, but only the coarse channel for VAE\n",
    "            unet_pred_coarse = pred[:, 0:1]\n",
    "            residual = residual.cpu()\n",
    "            fuzzy_inputs.append(fuzzy_input)\n",
    "            unet_preds.append(unet_pred_coarse)\n",
    "            residuals.append(residual)\n",
    "    fuzzy_inputs = torch.cat(fuzzy_inputs, dim=0)\n",
    "    unet_preds = torch.cat(unet_preds, dim=0)\n",
    "    residuals = torch.cat(residuals, dim=0)\n",
    "\n",
    "# When creating ResidualsDataModule, use fuzzy_inputs (full), but in VAELitModule._loss, use only the coarse channel for VAE encoder:\n",
    "# encoder_input = torch.cat([unet_pred[:, 0:1], residual], dim=1)\n",
    "\n",
    "    #print(f\"residual shape: {residuals.shape}\")\n",
    "    #print(\"residual nans:\", torch.isnan(residuals).any()) \n",
    "    # Sometimes debugging step to check whether residuals are nans is required if loss becomes nan in VAE\n",
    "\n",
    "    # VAE\n",
    "    vae_ckpt_dir = os.path.join(checkpoint_dir, \"vae\")\n",
    "    os.makedirs(vae_ckpt_dir, exist_ok=True)\n",
    "    vae_ckpts = sorted([f for f in os.listdir(vae_ckpt_dir) if f.endswith(\".ckpt\")])\n",
    "    if vae_ckpts:\n",
    "        print(\"Loading pretrained VAE ckpt\")\n",
    "        vae_module = VAELitModule.load_from_checkpoint(os.path.join(vae_ckpt_dir, vae_ckpts[-1]))\n",
    "    else:\n",
    "        print(\"Training VAE\")\n",
    "        vae_module = VAELitModule(\n",
    "            latent_dim=cfg.model.vae.latent_dim,\n",
    "            lr=cfg.model.vae.lr,\n",
    "            kl_weight=cfg.model.vae.kl_weight,\n",
    "            input_channels=cfg.model.vae.input_channels,\n",
    "            input_height=cfg.model.vae.input_height,\n",
    "            input_width=cfg.model.vae.input_width,\n",
    "            encoder_channels=cfg.model.vae.encoder_channels,\n",
    "            decoder_channels=cfg.model.vae.decoder_channels,\n",
    "            optimizer=cfg.optimizer.vae,\n",
    "            scheduler=cfg.scheduler.vae,\n",
    "            unet_module=unet_module\n",
    "        )\n",
    "        vae_checkpoint = ModelCheckpoint(\n",
    "            dirpath=vae_ckpt_dir,\n",
    "            filename=\"vae-{epoch:02d}\",\n",
    "            save_top_k=1,\n",
    "            monitor=\"val/loss\",\n",
    "            mode=\"min\",\n",
    "        )\n",
    "        residual_data_module = ResidualsDataModule(unet_preds, residuals, cfg.data.batch_size)\n",
    "        residual_data_module.setup()\n",
    "\n",
    "\n",
    "        trainer_vae = Trainer(\n",
    "            max_epochs=cfg.training.vae_epochs,\n",
    "            accelerator=cfg.trainer.accelerator,\n",
    "            enable_checkpointing=True,\n",
    "            logger=False,\n",
    "            default_root_dir=vae_ckpt_dir,\n",
    "            callbacks=[vae_checkpoint],\n",
    "        )\n",
    "        trainer_vae.fit(vae_module, datamodule=residual_data_module)\n",
    "\n",
    "\n",
    "\n",
    "    # LDM\n",
    "    ldm_module = LDMLitModule(\n",
    "        vae=vae_module,\n",
    "        latent_dim=cfg.model.ldm.latent_dim,\n",
    "        lr=cfg.model.ldm.lr,\n",
    "        num_timesteps=cfg.model.ldm.num_timesteps,\n",
    "        noise_schedule=cfg.model.ldm.noise_schedule,\n",
    "        loss_type=cfg.model.ldm.loss_type,\n",
    "        hidden_dim=cfg.model.ldm.hidden_dim,\n",
    "        num_layers=cfg.model.ldm.num_layers,\n",
    "        optimizer=cfg.optimizer.ldm,\n",
    "        scheduler=cfg.scheduler.ldm,\n",
    "    )\n",
    "\n",
    "\n",
    "    ldm_checkpoint = ModelCheckpoint(\n",
    "        dirpath=os.path.join(checkpoint_dir, \"ldm\"),\n",
    "        filename=\"ldm-{epoch:02d}\",\n",
    "        save_top_k=1,\n",
    "        monitor=\"val/loss\",\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    residual_data_module = ResidualsDataModule(unet_preds, residuals, cfg.data.batch_size)\n",
    "    residual_data_module.setup()\n",
    "    trainer_ldm = Trainer(\n",
    "        max_epochs=cfg.training.ldm_epochs,\n",
    "        accelerator=cfg.trainer.accelerator,\n",
    "        enable_checkpointing=True,\n",
    "        logger=False,\n",
    "        default_root_dir=os.path.join(checkpoint_dir, \"ldm\"),\n",
    "        callbacks=[ldm_checkpoint],\n",
    "    )\n",
    "    trainer_ldm.fit(ldm_module, datamodule=residual_data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3785db5",
   "metadata": {},
   "source": [
    "Writing the YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model\": {\n",
    "        \"unet\": {\n",
    "            \"in_channels\": 19,\n",
    "            \"out_channels\": 1,\n",
    "            \"lr\": 1e-4,\n",
    "            \"channels\": [32, 16],\n",
    "            \"kernel_size\": 3\n",
    "        },\n",
    "\n",
    "\n",
    "\n",
    "        \"vae\": {\n",
    "            \"latent_dim\": 32,\n",
    "            \"hidden_dim\": 128,\n",
    "            \"input_channels\": 2, #Input channels = predicted mean + residual\n",
    "            \"input_height\": 672,\n",
    "            \"input_width\": 576, #Dimensions of the input image\n",
    "            \"lr\": 1e-4,\n",
    "            \"kl_weight\": 0.001,\n",
    "            \"encoder_channels\": [8],\n",
    "            \"decoder_channels\": [8]\n",
    "        },\n",
    "\n",
    "\n",
    "\n",
    "        \"ldm\": {\n",
    "            \"latent_dim\": 36,\n",
    "            \"lr\": 1e-4,\n",
    "            \"num_timesteps\": 20,\n",
    "            \"noise_schedule\": \"linear\",\n",
    "            \"loss_type\": \"l2\",\n",
    "            \"hidden_dim\": 64,\n",
    "            \"num_layers\": 2\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"max_epochs\": 10,\n",
    "        \"min_delta\": 1e-4,\n",
    "        \"accelerator\": \"gpu\", # Changed from \"auto\" to \"gpu\"....make sure you run on a gpu, else it shall take ages\n",
    "        \"gradient_clip_val\": 1.0\n",
    "    },\n",
    "\n",
    "\n",
    "\n",
    "    \"training\": {\n",
    "        \"unet_epochs\": 20,\n",
    "        \"vae_epochs\": 20,\n",
    "        \"ldm_epochs\": 20\n",
    "    },\n",
    "\n",
    "\n",
    "    \"optimizer\": {\n",
    "        \"unet\": {\n",
    "            \"type\": \"Adam\",\n",
    "            \"lr\": 1e-4,\n",
    "            \"weight_decay\": 1e-4\n",
    "        },\n",
    "\n",
    "\n",
    "\n",
    "        \"vae\": {\n",
    "            \"type\": \"AdamW\",\n",
    "            \"lr\": 1e-4,\n",
    "            \"betas\": [0.5, 0.9],\n",
    "            \"weight_decay\": 1e-3\n",
    "        },\n",
    "\n",
    "\n",
    "        \"ldm\": {\n",
    "            \"type\": \"AdamW\",\n",
    "            \"lr\": 1e-4,\n",
    "            \"betas\": [0.5, 0.9],\n",
    "            \"weight_decay\": 1e-3\n",
    "        }\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"unet\": {\n",
    "            \"type\": \"ReduceLROnPlateau\",\n",
    "            \"patience\": 3,\n",
    "            \"factor\": 0.25,\n",
    "            \"monitor\": \"val/loss\"\n",
    "        },\n",
    "        \"vae\": {\n",
    "            \"type\": \"ReduceLROnPlateau\",\n",
    "            \"patience\": 3,\n",
    "            \"factor\": 0.25,\n",
    "            \"monitor\": \"val/recon_loss\"\n",
    "        },\n",
    "        \"ldm\": {\n",
    "            \"type\": \"ReduceLROnPlateau\",\n",
    "            \"patience\": 3,\n",
    "            \"factor\": 0.25,\n",
    "            \"monitor\": \"val/loss\"\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"data\": {\n",
    "        \"batch_size\": 32,\n",
    "        \"test_split\": 0.15,\n",
    "        \"val_split\": 0.15,\n",
    "        \"num_workers\": 1\n",
    "    },\n",
    "    \"paths\": {\n",
    "        \"checkpoint_dir\": \"/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/TA_work/checkpoints\",\n",
    "        \"output_dir\": \"/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/TA_work/outputs_dir\",\n",
    "        \"data_dir\": \"/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/TA_work/DiffScaler/data\",\n",
    "        \"static_dir\": \"/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/TA_work/DiffScaler/data/static_var\"\n",
    "    },\n",
    "    \"seed\": 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52dbb428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making dir\n",
    "os.makedirs(\"conf\", exist_ok=True)\n",
    "with open(\"conf/config_experiments.yaml\", \"w\") as f:\n",
    "    yaml.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7892500b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 48 samples from 2020.\n",
      "Using 48 samples from 2020.\n",
      "Loading pretrained UNet ckpt\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.48 GiB. GPU 0 has a total capacity of 19.62 GiB of which 476.88 MiB is free. Including non-PyTorch memory, this process has 19.05 GiB memory in use. Of the allocated memory 12.55 GiB is allocated by PyTorch, and 6.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m cfg \u001b[38;5;241m=\u001b[39m OmegaConf\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconf/config_experiments.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_hierarchy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 65\u001b[0m, in \u001b[0;36mtrain_hierarchy\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     62\u001b[0m sharp_target \u001b[38;5;241m=\u001b[39m sharp_target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 65\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43munet_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuzzy_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     residual \u001b[38;5;241m=\u001b[39m sharp_target \u001b[38;5;241m-\u001b[39m pred\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# Save the full fuzzy_input for UNet, but only the coarse channel for VAE\u001b[39;00m\n",
      "File \u001b[0;32m/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Downscaling_Models/.micromamba/envs/diffscaler/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Downscaling_Models/.micromamba/envs/diffscaler/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 9\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(h))\n\u001b[1;32m     11\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal(h)\n",
      "File \u001b[0;32m/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Downscaling_Models/.micromamba/envs/diffscaler/lib/python3.9/site-packages/torch/nn/functional.py:1701\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1699\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1700\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1701\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.48 GiB. GPU 0 has a total capacity of 19.62 GiB of which 476.88 MiB is free. Including non-PyTorch memory, this process has 19.05 GiB memory in use. Of the allocated memory 12.55 GiB is allocated by PyTorch, and 6.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "cfg = OmegaConf.load(\"conf/config_experiments.yaml\")\n",
    "train_hierarchy(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af1d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(cfg):\n",
    "    # Denormalisation using stats from the json file\n",
    "    stats_path = os.path.join(cfg.paths.output_dir, \"stats.json\")\n",
    "    with open(stats_path, \"r\") as f:\n",
    "        stats = json.load(f)\n",
    "    low_2mt_mean = stats[\"low_2mt_mean\"]\n",
    "    low_2mt_std = stats[\"low_2mt_std\"]\n",
    "\n",
    "    def denormalise(arr):\n",
    "        return arr * low_2mt_std + low_2mt_mean\n",
    "\n",
    "    # Instantiate and set up the LightningDataModule\n",
    "    data_module = DownscalingDataModule(\n",
    "        batch_size=cfg.data.batch_size,\n",
    "        val_frac=cfg.data.val_split,\n",
    "        test_frac=cfg.data.test_split,\n",
    "        num_workers=cfg.data.get(\"num_workers\", 1),\n",
    "        static_dir=cfg.paths.static_dir,\n",
    "        save_stats_json=os.path.join(cfg.paths.output_dir, \"stats.json\")\n",
    "    )\n",
    "    data_module.setup()\n",
    "    checkpoint_dir = cfg.paths.checkpoint_dir\n",
    "\n",
    "    # Load ckpts for UNet, VAE, LDM\n",
    "    unet_module = UNetLitModule.load_from_checkpoint(\n",
    "        os.path.join(checkpoint_dir, \"unet\", sorted(os.listdir(checkpoint_dir + \"/unet\"))[-1])  #chosing the last ckpt\n",
    "    )\n",
    "    vae_module = VAELitModule.load_from_checkpoint(\n",
    "        os.path.join(checkpoint_dir, \"vae\", sorted(os.listdir(checkpoint_dir + \"/vae\"))[-1])\n",
    "    )\n",
    "    ldm_module = LDMLitModule.load_from_checkpoint(\n",
    "        os.path.join(checkpoint_dir, \"ldm\", sorted(os.listdir(checkpoint_dir + \"/ldm\"))[-1]),\n",
    "        vae=vae_module\n",
    "    )\n",
    "\n",
    "    test_dataset = data_module.test_dataset\n",
    "\n",
    "    # Eval() mode for inference\n",
    "    unet_module.eval()\n",
    "    vae_module.eval()\n",
    "    ldm_module.eval()\n",
    "\n",
    "    # visualising a single random test sample\n",
    "\n",
    "    idx = random.randint(0, len(test_dataset)-1) #You can set the index (int) to compare\n",
    "    sample = test_dataset[idx]\n",
    "    if isinstance(sample, dict):\n",
    "        fuzzy_input = torch.cat([\n",
    "            F.interpolate(sample[\"low_2mT\"].unsqueeze(0), size=sample[\"high_2mT\"].shape[-2:], mode='bilinear', align_corners=False),\n",
    "            sample[\"dem\"].unsqueeze(0),\n",
    "            sample[\"lat\"].unsqueeze(0),\n",
    "            sample[\"lc\"].unsqueeze(0)\n",
    "        ], dim=1).squeeze(0)\n",
    "\n",
    "        sharp_target = sample[\"high_2mT\"]\n",
    "    else:\n",
    "        fuzzy_input, sharp_target = sample\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        device = next(unet_module.parameters()).device\n",
    "        fuzzy_input = fuzzy_input.unsqueeze(0).to(device)\n",
    "\n",
    "        unet_pred = unet_module.net(fuzzy_input)\n",
    "\n",
    "        input_height = vae_module.hparams.input_height // 2\n",
    "        input_width = vae_module.hparams.input_width // 2\n",
    "        latent_shape = (1, 1, input_height, input_width)\n",
    "\n",
    "        # Downsample unet_pred to latent spatial size\n",
    "        unet_pred_ds = F.interpolate(unet_pred, size=(input_height, input_width), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Generate 3 samples\n",
    "        generated_latents = []\n",
    "        for _ in range(3):\n",
    "            z_sample = ldm_module.sample(latent_shape, unet_pred_ds)\n",
    "            z_sample_flat = z_sample.view(z_sample.size(0), -1)\n",
    "            generated_latents.append(z_sample_flat)\n",
    "\n",
    "        generated_latents = torch.cat(generated_latents, dim=0)\n",
    "\n",
    "        # Decode latents to residuals\n",
    "        generated_residuals = vae_module.model.decode(generated_latents, unet_pred)\n",
    "\n",
    "        final_reconstructions = unet_pred + generated_residuals\n",
    "\n",
    "\n",
    "\n",
    "    all_imgs = [\n",
    "        denormalise(fuzzy_input[0, 0].cpu().numpy()),  # ERA5 predictor\n",
    "        denormalise(final_reconstructions[0, 0].cpu().numpy()),        # Sample 1\n",
    "        denormalise(final_reconstructions[1, 0].cpu().numpy()),        # Sample 2\n",
    "        denormalise(final_reconstructions[2, 0].cpu().numpy()),        # Sample 3\n",
    "        denormalise(sharp_target[0].cpu().numpy()),                    # Ground truth COSMO-CLM\n",
    "    ]\n",
    "    \n",
    "    titles = [\n",
    "        \"ERA5 2m input (bilinearly interpolated to high res)\",\n",
    "        \"Sample 1 of HR prediction\",\n",
    "        \"Sample 2 of HR prediction\",\n",
    "        \"Sample 3 of HR prediction\",\n",
    "        \"Ground truth (COSMO-CLM)\"\n",
    "    ]\n",
    "    vmin = min(img.min() for img in all_imgs)\n",
    "    vmax = max(img.max() for img in all_imgs)\n",
    "\n",
    "\n",
    "    # latlon extent for plotting\n",
    "\n",
    "    lat = sample[\"lat\"].cpu().numpy()[0]\n",
    "    lon = sample[\"lon\"].cpu().numpy()[0]\n",
    "    lat_min, lat_max = lat.min(), lat.max()\n",
    "    lon_min, lon_max = lon.min(), lon.max()\n",
    "    extent = [lon_min, lon_max, lat_min, lat_max]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(28, 6), constrained_layout=True)\n",
    "    images = []\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        im = ax.imshow(all_imgs[i], cmap='coolwarm', vmin=vmin, vmax=vmax, extent=extent, origin='lower')\n",
    "        ax.set_title(titles[i], fontsize=15, fontweight='bold')\n",
    "        ax.set_xlabel(\"Longitude\", fontsize=13)\n",
    "        ax.set_ylabel(\"Latitude\", fontsize=13)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=11)\n",
    "        ax.grid(False, which='both')\n",
    "        images.append(im)\n",
    "\n",
    "    cbar = fig.colorbar(images[0], ax=axes, orientation='horizontal', fraction=0.04, pad=0.08)\n",
    "    cbar.set_label(\"2m Temperature (degrees C)\", fontsize=15, fontweight='bold')\n",
    "    cbar.ax.tick_params(labelsize=13)\n",
    "\n",
    "    plt.suptitle(\"ERA5 low res + 3 LDM Samples + COSMO-CLM Ground Truth\", fontsize=20, y=1.05, fontweight='bold')\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d4f473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 48 samples from 2020.\n",
      "Using 48 samples from 2020.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'lon'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Now that the models are trained,,, how will you perform inference using the trained hierarchy?\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 92\u001b[0m, in \u001b[0;36minference\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# latlon extent for plotting\u001b[39;00m\n\u001b[1;32m     91\u001b[0m lat \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 92\u001b[0m lon \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     93\u001b[0m lat_min, lat_max \u001b[38;5;241m=\u001b[39m lat\u001b[38;5;241m.\u001b[39mmin(), lat\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m     94\u001b[0m lon_min, lon_max \u001b[38;5;241m=\u001b[39m lon\u001b[38;5;241m.\u001b[39mmin(), lon\u001b[38;5;241m.\u001b[39mmax()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lon'"
     ]
    }
   ],
   "source": [
    "#Now that the models are trained,,, how will you perform inference using the trained hierarchy?\n",
    "inference(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffscaler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
