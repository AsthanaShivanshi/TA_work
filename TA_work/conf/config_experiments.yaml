data:
  batch_size: 16
  num_workers: 1
  test_split: 0.15
  train_split: 0.7
  val_split: 0.15
model:
  ldm:
    hidden_dim: 64
    latent_dim: 32
    loss_type: l2
    lr: 0.0001
    noise_schedule: linear
    num_layers: 2
    num_timesteps: 20
  unet:
    channels:
    - 32
    - 16
    in_channels: 19
    kernel_size: 3
    lr: 0.0001
    out_channels: 1
  vae:
    decoder_channels:
    - 8
    encoder_channels:
    - 8
    input_channels: 1
    input_height: 672
    input_width: 576
    kl_weight: 0.001
    latent_dim: 32
    lr: 0.0001
optimizer:
  ldm:
    betas:
    - 0.5
    - 0.9
    lr: 0.0001
    type: AdamW
    weight_decay: 0.001
  unet:
    lr: 0.0001
    type: Adam
    weight_decay: 0.0001
  vae:
    betas:
    - 0.5
    - 0.9
    lr: 0.0001
    type: AdamW
    weight_decay: 0.001
paths:
  checkpoint_dir: /work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/TA_work/checkpoints
  data_dir: /work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/TA_work/DiffScaler/data
  output_dir: /work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/TA_work/outputs_dir
  static_dir: /work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/TA_work/DiffScaler/data/static_var
scheduler:
  ldm:
    factor: 0.25
    monitor: val/loss
    patience: 2
    type: ReduceLROnPlateau
  unet:
    factor: 0.25
    monitor: val/loss
    patience: 2
    type: ReduceLROnPlateau
  vae:
    factor: 0.25
    monitor: val/recon_loss
    patience: 2
    type: ReduceLROnPlateau
seed: 42
trainer:
  accelerator: gpu
  gradient_clip_val: 1.0
  max_epochs: 10
  min_delta: 0.0001
training:
  ldm_epochs: 10
  unet_epochs: 10
  vae_epochs: 10
