{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f60f02f",
   "metadata": {},
   "source": [
    "You are already acquainted with Pytorch now as a fanstastic library for writing, modifying, testing and scaling up the code. In this notebook, let us learn a cool wrapper library for Pytorch known as Pytorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9556bf29",
   "metadata": {},
   "source": [
    "A great resource to learn about the differences between Pytorch and Pytorch Lightning and how Pytorch Lightning makes your life easier : https://www.geeksforgeeks.org/deep-learning/pytorch-vs-pytorch-lightning/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee6fee",
   "metadata": {},
   "source": [
    "Please note that according to this TA, you can`t really appreciate the resurcefulness of Pytorch lightning without learning the basics of Pytorch. Hence, take this tutorial as a 10,000 ft overview of what Pytorch and Pytorch lightning actually do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1041bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from lightning import LightningModule, LightningDataModule, Trainer\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "import hydra\n",
    "import random\n",
    "import zstandard\n",
    "import os\n",
    "import glob\n",
    "import io\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "import rioxarray\n",
    "import json\n",
    "import yaml\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515752fd",
   "metadata": {},
   "source": [
    "The original paper this notebook is based on, can be accessed at https://gmd.copernicus.org/articles/18/2051/2025/ . There are originally 14 input channels to predict 2 chanels ultimately : temperature and wind speed. For the purposes of this notebook (due to storage limits and memory issues), we shall only be using low res 2m temperature to predict the high res 2m temperature target field, using hourly data from a single year (2020). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f58108d",
   "metadata": {},
   "source": [
    " But to supplement our endeavours in the form of giving more data to the model, we will also feed in static inputs (inputs that do not change or evolve over time) which are 1. digital elevation model (DEM), 2. land cover categories and 3. latitudinal bands. The functions to do so can be found in the preprocessing file, and we import them here for ease of analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78b635d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import load_and_normalise, decompress_zst_pt, get_file_list, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1105b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "class DownscalingDataset(Dataset):\n",
    "    def __init__(self, file_list, static_vars, low_2mt_mean, low_2mt_std):\n",
    "        self.file_list = file_list  # List of (high_file, low_file, date)\n",
    "        self.static_vars = static_vars  # for a list of static variables, refer the preprocessing file which loads and normalises these datasets\n",
    "        self.low_2mt_mean = low_2mt_mean\n",
    "        self.low_2mt_std = low_2mt_std\n",
    "        self.samples = []\n",
    "        # Each file contains 24 hourly samples\n",
    "        for hf, lf, date in self.file_list:\n",
    "            for hour in range(24):\n",
    "                self.samples.append((hf, lf, hour))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hf, lf, hour = self.samples[idx]\n",
    "        # Loading here high-res and low-res data for the given hour\n",
    "        high_data = decompress_zst_pt(hf)\n",
    "        low_data = decompress_zst_pt(lf)\n",
    "        # the high res and low res (upsampled) low res files are being loaded here\n",
    "        high_t2m = high_data[hour][\"2mT\"].float()  #[672, 576]\n",
    "        low_t2m = low_data[hour][\"2mT\"].float()    #[84, 72]. : which gives us a super resolution factor of 8!\n",
    "\n",
    "\n",
    "        high_t2m = high_t2m.unsqueeze(0)  #[1, 672, 576]\n",
    "        low_t2m = low_t2m.unsqueeze(0)    #[1, 84, 72]\n",
    "        # Static vars\n",
    "        dem = self.static_vars[\"dem\"].unsqueeze(0)  #[1, 672, 576]\n",
    "        lat = self.static_vars[\"lat\"].unsqueeze(0)  #[1, 672, 576]\n",
    "        lc = self.static_vars[\"lc\"]                 #[bands, 672, 576] ,,,by bands we mean land cover bands here\n",
    "\n",
    "        #Finally, this returns a dictionary with the following keys and values (variables)\n",
    "        return {\n",
    "            \"low_2mT\": low_t2m,        #[1, 84, 72]\n",
    "            \"high_2mT\": high_t2m,      #[1, 672, 576]\n",
    "            \"dem\": dem,                #[1, 672, 576]\n",
    "            \"lat\": lat,                #[1, 672, 576]\n",
    "            \"lc\": lc                   #[bands, 672, 576]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9cb8000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "class DownscalingDataModule(LightningDataModule):\n",
    "    def __init__(self, batch_size, val_frac, test_frac, num_workers, static_dir, save_stats_json):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.val_frac = val_frac\n",
    "        self.test_frac = test_frac\n",
    "        self.num_workers = num_workers\n",
    "        self.static_dir = static_dir\n",
    "        self.save_stats_json = save_stats_json\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        static_vars, stats = load_and_normalise(\n",
    "            self.static_dir,\n",
    "            val_frac=self.val_frac,\n",
    "            test_frac=self.test_frac,\n",
    "            save_stats_json=self.save_stats_json\n",
    "        )\n",
    "        file_list = get_file_list()\n",
    "        dataset = DownscalingDataset(\n",
    "            file_list, static_vars,\n",
    "            low_2mt_mean=stats[\"low_2mt_mean\"],\n",
    "            low_2mt_std=stats[\"low_2mt_std\"]\n",
    "        )\n",
    "        N = len(dataset)\n",
    "        n_val = int(self.val_frac * N)\n",
    "        n_test = int(self.test_frac * N)\n",
    "        n_train = N - n_val - n_test\n",
    "        self.train_set, self.val_set, self.test_set = random_split(\n",
    "            dataset, [n_train, n_val, n_test], generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, collate_fn=collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, collate_fn=collate_fn)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, collate_fn=collate_fn)\n",
    "\n",
    "    @property\n",
    "    def test_dataset(self):\n",
    "        return self.test_set.dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9440c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, channels, kernel_size):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels[0], kernel_size, padding=1)\n",
    "        self.conv2 = nn.Conv2d(channels[0], channels[1], kernel_size, padding=1)\n",
    "        self.final = nn.Conv2d(channels[1], out_channels, 1)\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.relu(self.conv2(h))\n",
    "        output = self.final(h)\n",
    "        return output\n",
    "\n",
    "\n",
    "class UNetLitModule(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        net: torch.nn.Module = None,\n",
    "        lr: float = 1e-3,\n",
    "        optimizer: dict = None,\n",
    "        scheduler: dict = None,\n",
    "        loss_fn: torch.nn.Module = None,\n",
    "        in_channels: int = 19,\n",
    "        out_channels: int = 1,\n",
    "        channels: list = [32, 16],\n",
    "        kernel_size: int = 3,\n",
    "        ckpt_path: str = None,\n",
    "        ignore_keys: list = []\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(logger=False, ignore=['net', 'loss_fn'])\n",
    "        self.net = net if net is not None else UNet(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            channels=channels,\n",
    "            kernel_size=kernel_size\n",
    "        )\n",
    "        self.loss_fn = loss_fn if loss_fn is not None else nn.MSELoss()\n",
    "        self.lr = lr\n",
    "        self.hparams.optimizer = optimizer\n",
    "        self.hparams.scheduler = scheduler\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def model_step(self, batch):\n",
    "        fuzzy_input, sharp_target = batch\n",
    "        pred = self.forward(fuzzy_input)\n",
    "        loss = self.loss_fn(pred, sharp_target)\n",
    "        return loss, pred\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, pred = self.model_step(batch)\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, pred = self.model_step(batch)\n",
    "        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, pred = self.model_step(batch)\n",
    "        self.log(\"test/loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_cfg = self.hparams.get(\"optimizer\", None)\n",
    "        sch_cfg = self.hparams.get(\"scheduler\", None)\n",
    "\n",
    "        if opt_cfg and opt_cfg[\"type\"] == \"AdamW\":\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                self.parameters(),\n",
    "                lr=opt_cfg.get(\"lr\", self.lr),\n",
    "                betas=tuple(opt_cfg.get(\"betas\", (0.5, 0.9))),\n",
    "                weight_decay=opt_cfg.get(\"weight_decay\", 1e-3)\n",
    "            )\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(\n",
    "                self.parameters(),\n",
    "                lr=opt_cfg.get(\"lr\", self.lr) if opt_cfg else self.lr,\n",
    "                weight_decay=opt_cfg.get(\"weight_decay\", 1e-4) if opt_cfg else 1e-4\n",
    "            )\n",
    "\n",
    "        if sch_cfg and sch_cfg[\"type\"] == \"ReduceLROnPlateau\":\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                patience=sch_cfg.get(\"patience\", 2),\n",
    "                factor=sch_cfg.get(\"factor\", 0.25)\n",
    "            )\n",
    "            return {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": scheduler,\n",
    "                    \"monitor\": sch_cfg.get(\"monitor\", \"val/loss\"),\n",
    "                    \"frequency\": 1,\n",
    "                },\n",
    "            }\n",
    "        else:\n",
    "            return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65978fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "class ResidualsDataModule(LightningDataModule):\n",
    "    def __init__(self, residuals, batch_size):\n",
    "        super().__init__()\n",
    "        self.residuals = residuals\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.dataset = TensorDataset(self.residuals, self.residuals)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.dataset, batch_size=self.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5068d396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(mean, log_var):\n",
    "    kl = 0.5 * (log_var.exp() + mean.square() - 1.0 - log_var)\n",
    "    return kl.mean()\n",
    "\n",
    "def standard_normal_sampling(mean, log_var, num=None):\n",
    "    std = log_var.mul(0.5).exp()\n",
    "    shape = mean.shape\n",
    "    if num is not None:\n",
    "        shape = shape[:1] + (num,) + shape[1:]\n",
    "        mean = mean[:, None, ...]\n",
    "        std = std[:, None, ...]\n",
    "    return mean + std * torch.randn(shape, device=mean.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8f1d2eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "class VAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim=32,\n",
    "        lr=1e-3,\n",
    "        kl_weight=0.001,\n",
    "        input_channels=1,\n",
    "        input_height=672,\n",
    "        input_width=576,\n",
    "        encoder_channels=[8],\n",
    "        decoder_channels=[8],\n",
    "        optimizer=None,\n",
    "        scheduler=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        encoder_channels = list(encoder_channels)\n",
    "        decoder_channels = list(decoder_channels)\n",
    "        input_height = int(input_height)\n",
    "        input_width = int(input_width)\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.kl_weight = kl_weight\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, encoder_channels[0], 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.encoded_size = encoder_channels[0] * (input_height // 2) * (input_width // 2)\n",
    "        self.fc_mu = nn.Linear(self.encoded_size, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.encoded_size, latent_dim)\n",
    "        self.fc_decode = nn.Linear(latent_dim, self.encoded_size)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, (decoder_channels[0], input_height // 2, input_width // 2)),\n",
    "            nn.ConvTranspose2d(decoder_channels[0], input_channels, 2, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # Uses the corrected sample_from_standard_normal\n",
    "        return standard_normal_sampling(mu, logvar)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.fc_decode(z)\n",
    "        return self.decoder(h)\n",
    "\n",
    "    def forward(self, x, sample_posterior=True):\n",
    "        mu, logvar = self.encode(x)\n",
    "        if sample_posterior:\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "        else:\n",
    "            z = mu\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class VAELitModule(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim=32,\n",
    "        lr=1e-3,\n",
    "        kl_weight=0.001,\n",
    "        input_channels=1,\n",
    "        input_height=672,\n",
    "        input_width=576,\n",
    "        encoder_channels=[8],\n",
    "        decoder_channels=[8],\n",
    "        optimizer=None,\n",
    "        scheduler=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = VAE(\n",
    "            latent_dim=latent_dim,\n",
    "            lr=lr,\n",
    "            kl_weight=kl_weight,\n",
    "            input_channels=input_channels,\n",
    "            input_height=input_height,\n",
    "            input_width=input_width,\n",
    "            encoder_channels=encoder_channels,\n",
    "            decoder_channels=decoder_channels,\n",
    "        )\n",
    "        self.save_hyperparameters(logger=False)\n",
    "        self.lr = lr\n",
    "        self.kl_weight = kl_weight\n",
    "        self.hparams.optimizer = optimizer\n",
    "        self.hparams.scheduler = scheduler\n",
    "\n",
    "    def forward(self, x, sample_posterior=True):\n",
    "        return self.model(x, sample_posterior)\n",
    "\n",
    "    def _loss(self, batch):\n",
    "        noisy, clean = batch\n",
    "        recon, mu, logvar = self.forward(clean)\n",
    "\n",
    "        # Recon loss\n",
    "        recon_loss = F.l1_loss(recon, clean)\n",
    "\n",
    "        # KL divergence loss\n",
    "        kl_loss = kl_divergence(mu, logvar)\n",
    "\n",
    "        total_loss = recon_loss + self.kl_weight * kl_loss\n",
    "\n",
    "        return total_loss, recon_loss, kl_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        total_loss, recon_loss, kl_loss = self._loss(batch)\n",
    "\n",
    "        self.log(\"train/loss\", total_loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"train/recon_loss\", recon_loss, on_step=True, on_epoch=True, sync_dist=True)\n",
    "        self.log(\"train/kl_loss\", kl_loss, on_step=True, on_epoch=True, sync_dist=True)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        total_loss, recon_loss, kl_loss = self._loss(batch)\n",
    "\n",
    "        self.log(\"val/loss\", total_loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"val/recon_loss\", recon_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
    "        self.log(\"val/kl_loss\", kl_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        total_loss, recon_loss, kl_loss = self._loss(batch)\n",
    "\n",
    "        self.log(\"test/loss\", total_loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"test/recon_loss\", recon_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
    "        self.log(\"test/kl_loss\", kl_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Example: using self.hparams for optimizer/scheduler settings from config\n",
    "        opt_cfg = self.hparams.get(\"optimizer\", None)\n",
    "        sch_cfg = self.hparams.get(\"scheduler\", None)\n",
    "\n",
    "        if opt_cfg and opt_cfg[\"type\"] == \"AdamW\":\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                self.parameters(),\n",
    "                lr=opt_cfg.get(\"lr\", self.lr),\n",
    "                betas=tuple(opt_cfg.get(\"betas\", (0.5, 0.9))),\n",
    "                weight_decay=opt_cfg.get(\"weight_decay\", 1e-3)\n",
    "            )\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(\n",
    "                self.parameters(),\n",
    "                lr=opt_cfg.get(\"lr\", self.lr) if opt_cfg else self.lr,\n",
    "                weight_decay=opt_cfg.get(\"weight_decay\", 1e-4) if opt_cfg else 1e-4\n",
    "            )\n",
    "\n",
    "        if sch_cfg and sch_cfg[\"type\"] == \"ReduceLROnPlateau\":\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                patience=sch_cfg.get(\"patience\", 3),\n",
    "                factor=sch_cfg.get(\"factor\", 0.25)\n",
    "            )\n",
    "            return {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": scheduler,\n",
    "                    \"monitor\": sch_cfg.get(\"monitor\", \"val/recon_loss\"),\n",
    "                    \"frequency\": 1,\n",
    "                },\n",
    "            }\n",
    "        else:\n",
    "            return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dfbbf67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "class LatentDenoiser(nn.Module):\n",
    "    def __init__(self, latent_dim=64, hidden_dim=128, num_layers=4):  # always make sure to have a config file ,, these are just default in case no information is provided in config.\n",
    "    #We will talk about config files later in the exercise at the bottom, wherre everything comes togetehr and makes sense.\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(1, latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dim, latent_dim)\n",
    "        )\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(latent_dim * 2, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.LayerNorm(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.1))\n",
    "        layers.append(nn.Linear(hidden_dim, latent_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z_noisy, timestep=None):\n",
    "        if timestep is None:\n",
    "            timestep = torch.zeros(z_noisy.shape[0], 1, device=z_noisy.device)\n",
    "        if timestep.max() > 1.0:\n",
    "            timestep = timestep / 1000.0\n",
    "        t_embed = self.time_embed(timestep.float())\n",
    "        x = torch.cat([z_noisy, t_embed], dim=-1)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LDMLitModule(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vae,\n",
    "        latent_dim=64,\n",
    "        lr=1e-4,\n",
    "        num_timesteps=50,\n",
    "        noise_schedule=\"linear\",\n",
    "        loss_type=\"l2\",\n",
    "        hidden_dim=128,\n",
    "        num_layers=4,\n",
    "        optimizer=None,\n",
    "        scheduler=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Save all hyperparameters except vae\n",
    "        self.save_hyperparameters(logger=False, ignore=['vae'])\n",
    "        self.vae = vae.model.requires_grad_(False)\n",
    "        self.vae.eval()\n",
    "        # Pass config values to LatentDenoiser\n",
    "        self.denoiser = LatentDenoiser(latent_dim, hidden_dim, num_layers)\n",
    "        self.lr = lr\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.loss_type = loss_type\n",
    "        self.register_noise_schedule(noise_schedule)\n",
    "        if loss_type == \"l1\":\n",
    "            self.loss_fn = nn.L1Loss()\n",
    "        elif loss_type == \"l2\":\n",
    "            self.loss_fn = nn.MSELoss()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss type: {loss_type}\")\n",
    "\n",
    "    def register_noise_schedule(self, schedule=\"linear\"):\n",
    "        if schedule == \"linear\":\n",
    "            betas = torch.linspace(1e-4, 2e-2, self.num_timesteps)\n",
    "        elif schedule == \"cosine\":\n",
    "            timesteps = torch.arange(self.num_timesteps + 1) / self.num_timesteps\n",
    "            alphas = torch.cos(timesteps * torch.pi / 2) ** 2\n",
    "            alphas = alphas / alphas[0]\n",
    "            betas = 1 - alphas[1:] / alphas[:-1]\n",
    "            betas = torch.clamp(betas, 0, 0.999)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown schedule: {schedule}\")\n",
    "        alphas = 1 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1 - alphas_cumprod))\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].view(-1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1)\n",
    "        return (sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise)\n",
    "\n",
    "    def p_losses(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        x_noisy = self.q_sample(x_start, t, noise)\n",
    "        predicted_noise = self.denoiser(x_noisy, t.unsqueeze(1))\n",
    "        loss = self.loss_fn(predicted_noise, noise)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        with torch.no_grad():\n",
    "            mu, logvar = self.vae.encode(x)\n",
    "            z = self.vae.reparameterize(mu, logvar)\n",
    "        t = torch.randint(0, self.num_timesteps, (batch_size,), device=self.device)\n",
    "        loss = self.p_losses(z, t)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        noisy, clean = batch\n",
    "        loss = self.forward(clean)\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        noisy, clean = batch\n",
    "        loss = self.forward(clean)\n",
    "        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        noisy, clean = batch\n",
    "        loss = self.forward(clean)\n",
    "        self.log(\"test/loss\", loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, shape, num_steps=50):\n",
    "        z = torch.randn(shape, device=self.device)\n",
    "        timesteps = torch.linspace(self.num_timesteps-1, 0, num_steps, dtype=torch.long, device=self.device)\n",
    "        for i, t in enumerate(timesteps):\n",
    "            t_batch = t.repeat(shape[0])\n",
    "            predicted_noise = self.denoiser(z, t_batch.unsqueeze(1))\n",
    "            alpha_t = self.alphas_cumprod[t]\n",
    "            alpha_t_prev = self.alphas_cumprod[t-1] if t > 0 else torch.tensor(1.0)\n",
    "            pred_x0 = (z - torch.sqrt(1 - alpha_t) * predicted_noise) / torch.sqrt(alpha_t)\n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(z) if i < len(timesteps) - 1 else 0\n",
    "                z = (torch.sqrt(alpha_t_prev) * pred_x0 + torch.sqrt(1 - alpha_t_prev) * noise)\n",
    "            else:\n",
    "                z = pred_x0\n",
    "        return z\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_samples(self, num_samples=1):\n",
    "        latent_shape = (num_samples, self.hparams.latent_dim)\n",
    "        z_samples = self.sample(latent_shape)\n",
    "        samples = self.vae.decode(z_samples)\n",
    "        return samples\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_cfg = self.hparams.get(\"optimizer\", None)\n",
    "        sch_cfg = self.hparams.get(\"scheduler\", None)\n",
    "\n",
    "        if opt_cfg and opt_cfg[\"type\"] == \"AdamW\":\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                self.parameters(),\n",
    "                lr=opt_cfg.get(\"lr\", self.lr),\n",
    "                betas=tuple(opt_cfg.get(\"betas\", (0.5, 0.9))),\n",
    "                weight_decay=opt_cfg.get(\"weight_decay\", 1e-3)\n",
    "            )\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(\n",
    "                self.parameters(),\n",
    "                lr=opt_cfg.get(\"lr\", self.lr) if opt_cfg else self.lr,\n",
    "                weight_decay=opt_cfg.get(\"weight_decay\", 1e-4) if opt_cfg else 1e-4\n",
    "            )\n",
    "\n",
    "        if sch_cfg and sch_cfg[\"type\"] == \"ReduceLROnPlateau\":\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                patience=sch_cfg.get(\"patience\", 5),\n",
    "                factor=sch_cfg.get(\"factor\", 0.5)\n",
    "            )\n",
    "            return {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\n",
    "                    \"scheduler\": scheduler,\n",
    "                    \"monitor\": sch_cfg.get(\"monitor\", \"val/loss\"),\n",
    "                    \"frequency\": 1,\n",
    "                },\n",
    "            }\n",
    "        else:\n",
    "            return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5a1040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hierarchy(cfg):\n",
    "    data_module = DownscalingDataModule(\n",
    "        batch_size=cfg.data.batch_size,\n",
    "        val_frac=cfg.data.val_split,\n",
    "        test_frac=cfg.data.test_split,\n",
    "        num_workers=cfg.data.num_workers,\n",
    "        static_dir=cfg.paths.static_dir,\n",
    "        save_stats_json=os.path.join(cfg.paths.output_dir, \"stats.json\")\n",
    "    )\n",
    "    data_module.setup()\n",
    "\n",
    "    checkpoint_dir = cfg.paths.checkpoint_dir\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # UNet\n",
    "    unet_module = UNetLitModule(\n",
    "    net=UNet(\n",
    "        in_channels=cfg.model.unet.in_channels,\n",
    "        out_channels=cfg.model.unet.out_channels,\n",
    "        channels=cfg.model.unet.channels,\n",
    "        kernel_size=cfg.model.unet.kernel_size,\n",
    "    ),\n",
    "    lr=cfg.model.unet.lr,\n",
    "    optimizer=cfg.optimizer.unet,\n",
    "    scheduler=cfg.scheduler.unet,\n",
    ")\n",
    "    unet_checkpoint = ModelCheckpoint(\n",
    "        dirpath=checkpoint_dir + \"/unet\",\n",
    "        filename=\"unet-{epoch:02d}\",\n",
    "        save_top_k=1,\n",
    "        monitor=\"val/loss\",\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    trainer_unet = Trainer(\n",
    "        max_epochs=cfg.training.unet_epochs,\n",
    "        accelerator=cfg.trainer.accelerator,\n",
    "        gradient_clip_val=cfg.trainer.get(\"gradient_clip_val\", 1.0),\n",
    "        enable_checkpointing=True,\n",
    "        logger=False,\n",
    "        callbacks=[unet_checkpoint],\n",
    "    )\n",
    "    trainer_unet.fit(unet_module, datamodule=data_module)\n",
    "\n",
    "    # Residuals\n",
    "    residuals = []\n",
    "    for batch in data_module.train_dataloader():\n",
    "        fuzzy_input, sharp_target = batch\n",
    "        with torch.no_grad():\n",
    "            pred = unet_module.net(fuzzy_input)\n",
    "            residual = sharp_target - pred\n",
    "            residuals.append(residual)\n",
    "    residuals = torch.cat(residuals, dim=0)\n",
    "    print(f\"Residuals shape: {residuals.shape}\") #This is a debug step, as the VAE training had nans\n",
    "    print(\"residual nans:\",torch.isnan(residuals).any())\n",
    "\n",
    "    # VAE\n",
    "    vae_module = VAELitModule(\n",
    "        latent_dim=cfg.model.vae.latent_dim,\n",
    "        lr=cfg.model.vae.lr,\n",
    "        kl_weight=cfg.model.vae.kl_weight,\n",
    "        input_channels=cfg.model.vae.input_channels,\n",
    "        input_height=cfg.model.vae.input_height,\n",
    "        input_width=cfg.model.vae.input_width,\n",
    "        encoder_channels=cfg.model.vae.encoder_channels,\n",
    "        decoder_channels=cfg.model.vae.decoder_channels,\n",
    "        optimizer=cfg.optimizer.vae,\n",
    "        scheduler=cfg.scheduler.vae,\n",
    "    )\n",
    "\n",
    "    vae_checkpoint = ModelCheckpoint(\n",
    "        dirpath=checkpoint_dir + \"/vae\",\n",
    "        filename=\"vae-{epoch:02d}\",\n",
    "        save_top_k=1,\n",
    "        monitor=\"val/loss\",\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    residual_data_module = ResidualsDataModule(residuals, cfg.data.batch_size)\n",
    "    residual_data_module.setup()\n",
    "\n",
    "\n",
    "    trainer_vae = Trainer(\n",
    "        max_epochs=cfg.training.vae_epochs,\n",
    "        accelerator=cfg.trainer.accelerator,\n",
    "        enable_checkpointing=True,\n",
    "        logger=False,\n",
    "        default_root_dir=checkpoint_dir + \"/vae\",\n",
    "        callbacks=[vae_checkpoint],\n",
    "    )\n",
    "    trainer_vae.fit(vae_module, datamodule=residual_data_module)\n",
    "\n",
    "    # LDM\n",
    "    ldm_module = LDMLitModule(\n",
    "        vae=vae_module,\n",
    "        latent_dim=cfg.model.ldm.latent_dim,\n",
    "        lr=cfg.model.ldm.lr,\n",
    "        num_timesteps=cfg.model.ldm.num_timesteps,\n",
    "        noise_schedule=cfg.model.ldm.noise_schedule,\n",
    "        loss_type=cfg.model.ldm.loss_type,\n",
    "        hidden_dim=cfg.model.ldm.hidden_dim,\n",
    "        num_layers=cfg.model.ldm.num_layers,\n",
    "        optimizer=cfg.optimizer.ldm,\n",
    "        scheduler=cfg.scheduler.ldm,\n",
    "    )\n",
    "    ldm_checkpoint = ModelCheckpoint(\n",
    "        dirpath=checkpoint_dir + \"/ldm\",\n",
    "        filename=\"ldm-{epoch:02d}\",\n",
    "        save_top_k=1,\n",
    "        monitor=\"val/loss\",\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    trainer_ldm = Trainer(\n",
    "        max_epochs=cfg.training.ldm_epochs,\n",
    "        accelerator=cfg.trainer.accelerator,\n",
    "        enable_checkpointing=True,\n",
    "        logger=False,\n",
    "        default_root_dir=checkpoint_dir + \"/ldm\",\n",
    "        callbacks=[ldm_checkpoint],\n",
    "    )\n",
    "    trainer_ldm.fit(ldm_module, datamodule=residual_data_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title\n",
    "\n",
    "config = {\n",
    "    \"model\": {\n",
    "        \"unet\": {\n",
    "            \"in_channels\": 19,\n",
    "            \"out_channels\": 1,\n",
    "            \"lr\": 1e-4,\n",
    "            \"channels\": [32, 16],\n",
    "            \"kernel_size\": 3\n",
    "        },\n",
    "        \"vae\": {\n",
    "            \"latent_dim\": 32,\n",
    "            \"input_channels\": 1,\n",
    "            \"input_height\": 672,\n",
    "            \"input_width\": 576, #Dimensions of the input image\n",
    "            \"lr\": 1e-4,\n",
    "            \"kl_weight\": 0.001,\n",
    "            \"encoder_channels\": [8],\n",
    "            \"decoder_channels\": [8]\n",
    "        },\n",
    "        \"ldm\": {\n",
    "            \"latent_dim\": 32,\n",
    "            \"lr\": 1e-4,\n",
    "            \"num_timesteps\": 20,\n",
    "            \"noise_schedule\": \"linear\",\n",
    "            \"loss_type\": \"l2\",\n",
    "            \"hidden_dim\": 64,\n",
    "            \"num_layers\": 2\n",
    "        }\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"max_epochs\": 10,\n",
    "        \"min_delta\": 1e-4,\n",
    "        \"accelerator\": \"gpu\", # Changed from \"auto\" to \"gpu\"....make sure you run on a gpu, else it shall take ages\n",
    "        \"gradient_clip_val\": 1.0\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"unet_epochs\": 10,\n",
    "        \"vae_epochs\": 10,\n",
    "        \"ldm_epochs\": 10\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"unet\": {\n",
    "            \"type\": \"Adam\",\n",
    "            \"lr\": 1e-4,\n",
    "            \"weight_decay\": 1e-4\n",
    "        },\n",
    "        \"vae\": {\n",
    "            \"type\": \"AdamW\",\n",
    "            \"lr\": 1e-4,\n",
    "            \"betas\": [0.5, 0.9],\n",
    "            \"weight_decay\": 1e-3\n",
    "        },\n",
    "        \"ldm\": {\n",
    "            \"type\": \"AdamW\",\n",
    "            \"lr\": 1e-4,\n",
    "            \"betas\": [0.5, 0.9],\n",
    "            \"weight_decay\": 1e-3\n",
    "        }\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"unet\": {\n",
    "            \"type\": \"ReduceLROnPlateau\",\n",
    "            \"patience\": 2,\n",
    "            \"factor\": 0.25,\n",
    "            \"monitor\": \"val/loss\"\n",
    "        },\n",
    "        \"vae\": {\n",
    "            \"type\": \"ReduceLROnPlateau\",\n",
    "            \"patience\": 2,\n",
    "            \"factor\": 0.25,\n",
    "            \"monitor\": \"val/recon_loss\"\n",
    "        },\n",
    "        \"ldm\": {\n",
    "            \"type\": \"ReduceLROnPlateau\",\n",
    "            \"patience\": 2,\n",
    "            \"factor\": 0.25,\n",
    "            \"monitor\": \"val/loss\"\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"data\": {\n",
    "        \"batch_size\": 16,\n",
    "        \"test_split\": 0.15,\n",
    "        \"val_split\": 0.15,\n",
    "        \"num_workers\": 1\n",
    "    },\n",
    "    \"paths\": {\n",
    "        \"checkpoint_dir\": \"/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/TA_work/checkpoints\",\n",
    "        \"output_dir\": \"/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/TA_work/outputs_dir\",\n",
    "        \"data_dir\": \"/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/TA_work/DiffScaler/data\",\n",
    "        \"static_dir\": \"/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/TA_work/DiffScaler/data/static_var\"\n",
    "    },\n",
    "    \"seed\": 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "52dbb428",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"conf/config_experiments.yaml\", \"w\") as f:\n",
    "    yaml.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7892500b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 24 of 48 files from 2020.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Downscaling_Models/.micromamba/envs/diffscaler/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/D ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 24 of 48 files from 2020.\n",
      "Using 24 of 48 files from 2020.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Downscaling_Models/.micromamba/envs/diffscaler/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:701: Checkpoint directory /work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/TA_work/checkpoints/unet exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | net     | UNet    | 10.1 K | train\n",
      "1 | loss_fn | MSELoss | 0      | train\n",
      "--------------------------------------------\n",
      "10.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "10.1 K    Total params\n",
      "0.041     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 24 of 48 files from 2020.\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Downscaling_Models/.micromamba/envs/diffscaler/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Downscaling_Models/.micromamba/envs/diffscaler/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 26/26 [00:32<00:00,  0.79it/s, train/loss_step=0.0452, val/loss=0.0522, train/loss_epoch=0.0544]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 26/26 [00:32<00:00,  0.79it/s, train/loss_step=0.0452, val/loss=0.0522, train/loss_epoch=0.0544]\n",
      "Residuals shape: torch.Size([404, 1, 672, 576])\n",
      "residual nans: tensor(False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Downscaling_Models/.micromamba/envs/diffscaler/lib/python3.9/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/D ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params | Mode \n",
      "---------------------------------------\n",
      "0 | model | VAE  | 75.1 M | train\n",
      "---------------------------------------\n",
      "75.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "75.1 M    Total params\n",
      "300.369   Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 26/26 [00:22<00:00,  1.16it/s, train/loss_step=0.237, val/loss=0.218, train/loss_epoch=0.232]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 26/26 [00:23<00:00,  1.10it/s, train/loss_step=0.237, val/loss=0.218, train/loss_epoch=0.232]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type           | Params | Mode \n",
      "----------------------------------------------------\n",
      "0 | vae      | VAE            | 75.1 M | eval \n",
      "1 | denoiser | LatentDenoiser | 7.4 K  | train\n",
      "2 | loss_fn  | MSELoss        | 0      | train\n",
      "----------------------------------------------------\n",
      "7.4 K     Trainable params\n",
      "75.1 M    Non-trainable params\n",
      "75.1 M    Total params\n",
      "300.398   Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "13        Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 26/26 [00:20<00:00,  1.28it/s, train/loss_step=0.900, val/loss=1.000, train/loss_epoch=0.998]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 26/26 [00:20<00:00,  1.28it/s, train/loss_step=0.900, val/loss=1.000, train/loss_epoch=0.998]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    cfg = OmegaConf.load(\"conf/config_experiments.yaml\")\n",
    "    train_hierarchy(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8af1d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(cfg):\n",
    "    # Load stats for denormalisation\n",
    "    stats_path = \"outputs_dir/stats.json\"\n",
    "    with open(stats_path, \"r\") as f:\n",
    "        stats = json.load(f)\n",
    "    low_2mt_mean = stats[\"low_2mt_mean\"]\n",
    "    low_2mt_std = stats[\"low_2mt_std\"]\n",
    "\n",
    "    def denormalise(arr):\n",
    "        return arr * low_2mt_std + low_2mt_mean\n",
    "\n",
    "    # Instantiate and set up the LightningDataModule\n",
    "    data_module = DownscalingDataModule(\n",
    "        batch_size=cfg.data.batch_size,\n",
    "        val_frac=cfg.data.val_split,\n",
    "        test_frac=cfg.data.test_split,\n",
    "        num_workers=cfg.data.get(\"num_workers\", 1),\n",
    "        static_dir=cfg.paths.static_dir,\n",
    "        save_stats_json=os.path.join(cfg.paths.output_dir, \"stats.json\")\n",
    "    )\n",
    "    data_module.setup()\n",
    "    checkpoint_dir = cfg.paths.checkpoint_dir\n",
    "\n",
    "    # Load checkpoints from each step of the hierarchy trained above\n",
    "    unet_module = UNetLitModule.load_from_checkpoint(\n",
    "        os.path.join(checkpoint_dir, \"unet\", sorted(os.listdir(checkpoint_dir + \"/unet\"))[-1])  #chosing the last ckpt\n",
    "    )\n",
    "    vae_module = VAELitModule.load_from_checkpoint(\n",
    "        os.path.join(checkpoint_dir, \"vae\", sorted(os.listdir(checkpoint_dir + \"/vae\"))[-1])\n",
    "    )\n",
    "    ldm_module = LDMLitModule.load_from_checkpoint(\n",
    "        os.path.join(checkpoint_dir, \"ldm\", sorted(os.listdir(checkpoint_dir + \"/ldm\"))[-1]),\n",
    "        vae=vae_module\n",
    "    )\n",
    "\n",
    "    test_dataset = data_module.test_dataset\n",
    "\n",
    "    # All three models in eval() mode\n",
    "    unet_module.eval()\n",
    "    vae_module.eval()\n",
    "    ldm_module.eval()\n",
    "\n",
    "    # visualising a single random test sample\n",
    "    idx = random.randint(0, len(test_dataset)-1)\n",
    "    sample = test_dataset[idx]\n",
    "    if isinstance(sample, dict):\n",
    "        fuzzy_input = torch.cat([\n",
    "            F.interpolate(sample[\"low_2mT\"].unsqueeze(0), size=sample[\"high_2mT\"].shape[-2:], mode='bilinear', align_corners=False),\n",
    "            sample[\"dem\"].unsqueeze(0),\n",
    "            sample[\"lat\"].unsqueeze(0),\n",
    "            sample[\"lc\"].unsqueeze(0)\n",
    "        ], dim=1).squeeze(0)\n",
    "        sharp_target = sample[\"high_2mT\"]\n",
    "    else:\n",
    "        fuzzy_input, sharp_target = sample\n",
    "\n",
    "    with torch.no_grad():\n",
    "        device = next(unet_module.parameters()).device\n",
    "        fuzzy_input = fuzzy_input.unsqueeze(0).to(device)\n",
    "        unet_pred = unet_module.net(fuzzy_input)\n",
    "        # Generate 3 samples of residuals\n",
    "        generated_residuals = ldm_module.generate_samples(num_samples=3)\n",
    "        final_reconstructions = unet_pred + generated_residuals\n",
    "        final_reconstructions = final_reconstructions.clamp(0, 1)\n",
    "\n",
    "    plt.figure(figsize=(30, 5))\n",
    "    plt.subplot(1, 6, 1)\n",
    "    plt.title(\"Original High-Res\")\n",
    "    plt.imshow(denormalise(sharp_target[0].cpu().numpy()), cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 6, 2)\n",
    "    plt.title(\"Low-Res Input (Upsampled)\")\n",
    "    plt.imshow(denormalise(fuzzy_input[0, 0].cpu().numpy()), cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 6, 3)\n",
    "    plt.title(\"UNet Prediction\")\n",
    "    plt.imshow(denormalise(unet_pred[0, 0].cpu().numpy()), cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Three generated samples\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 6, 4+i)\n",
    "        plt.title(f\"Sample {i+1}\")\n",
    "        plt.imshow(denormalise(final_reconstructions[i, 0].cpu().numpy()), cmap='viridis')\n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.suptitle(\"Single Test Sample: 3 Generated High-Res Outputs\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "85d4f473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 24 of 48 files from 2020.\n",
      "Using 24 of 48 files from 2020.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'channels' and 'kernel_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Now that the models are trained,,, how will you perform inference using the trained hierarchy?\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[52], line 25\u001b[0m, in \u001b[0;36minference\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     22\u001b[0m checkpoint_dir \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mpaths\u001b[38;5;241m.\u001b[39mcheckpoint_dir\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Load checkpoints from each step of the hierarchy trained above\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m unet_module \u001b[38;5;241m=\u001b[39m \u001b[43mUNetLitModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/unet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#chosing the last ckpt\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m vae_module \u001b[38;5;241m=\u001b[39m VAELitModule\u001b[38;5;241m.\u001b[39mload_from_checkpoint(\n\u001b[1;32m     29\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvae\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(checkpoint_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/vae\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m ldm_module \u001b[38;5;241m=\u001b[39m LDMLitModule\u001b[38;5;241m.\u001b[39mload_from_checkpoint(\n\u001b[1;32m     32\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mldm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(checkpoint_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/ldm\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]),\n\u001b[1;32m     33\u001b[0m     vae\u001b[38;5;241m=\u001b[39mvae_module\n\u001b[1;32m     34\u001b[0m )\n",
      "File \u001b[0;32m/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Downscaling_Models/.micromamba/envs/diffscaler/lib/python3.9/site-packages/lightning/pytorch/utilities/model_helpers.py:125\u001b[0m, in \u001b[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be called on an instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m     )\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Downscaling_Models/.micromamba/envs/diffscaler/lib/python3.9/site-packages/lightning/pytorch/core/module.py:1662\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[1;32m   1574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1575\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1581\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1582\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;124;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[1;32m   1584\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1660\u001b[0m \n\u001b[1;32m   1661\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1662\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Downscaling_Models/.micromamba/envs/diffscaler/lib/python3.9/site-packages/lightning/pytorch/core/saving.py:91\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[0;32m---> 91\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state_dict:\n",
      "File \u001b[0;32m/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Downscaling_Models/.micromamba/envs/diffscaler/lib/python3.9/site-packages/lightning/pytorch/core/saving.py:165\u001b[0m, in \u001b[0;36m_load_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cls_spec\u001b[38;5;241m.\u001b[39mvarkw:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# filter kwargs according to class init unless it allows any argument via kwargs\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     _cls_kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _cls_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m cls_init_args_name}\n\u001b[0;32m--> 165\u001b[0m obj \u001b[38;5;241m=\u001b[39m instantiator(\u001b[38;5;28mcls\u001b[39m, _cls_kwargs) \u001b[38;5;28;01mif\u001b[39;00m instantiator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_cls_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, pl\u001b[38;5;241m.\u001b[39mLightningDataModule):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m checkpoint:\n",
      "Cell \u001b[0;32mIn[34], line 28\u001b[0m, in \u001b[0;36mUNetLitModule.__init__\u001b[0;34m(self, net, lr, optimizer, scheduler, loss_fn, ckpt_path, ignore_keys)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_hyperparameters(logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, ignore\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnet\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_fn\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet \u001b[38;5;241m=\u001b[39m net \u001b[38;5;28;01mif\u001b[39;00m net \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mUNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m19\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn \u001b[38;5;241m=\u001b[39m loss_fn \u001b[38;5;28;01mif\u001b[39;00m loss_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m=\u001b[39m lr\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'channels' and 'kernel_size'"
     ]
    }
   ],
   "source": [
    "#Now that the models are trained,,, how will you perform inference using the trained hierarchy?\n",
    "\n",
    "inference(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1080d0a6",
   "metadata": {},
   "source": [
    "Part II : Hydra : the one stop solution to all your experimental needs !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa48fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "@hydra.main(config_path=\"conf\", config_name=\"config_experiments.yaml\")\n",
    "def main(cfg: DictConfig):\n",
    "    print(OmegaConf.to_yaml(cfg))\n",
    "    train_hierarchy(cfg)\n",
    "    inference(cfg)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffscaler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
